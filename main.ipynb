{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сбор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install bs4\n",
    "%pip install requests \n",
    "%pip install fake_useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from fake_useragent import UserAgent\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent()\n",
    "session = requests.session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(block: typing.Dict[str, str]) -> typing.Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Преобразует данные блока информации.\n",
    "\n",
    "    Args:\n",
    "        block (Dict[str, str]): Словарь с данными блока информации.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: Преобразованный словарь с данными.\n",
    "    \"\"\"\n",
    "    \n",
    "     # Обход всех ключей и значений в словаре block\n",
    "    for k, v in block.items():\n",
    "        try:\n",
    "            # Проверка ключа на принадлежность к определенным категориям\n",
    "            if k in ['URL', 'название источника']:\n",
    "                pass\n",
    "            elif k == 'текст статьи':\n",
    "                # Обработка текста статьи: объединение и очистка текстовых элементов\n",
    "                block[k] = ''.join([i.text.strip() for i in v]).replace('\\xa0', ' ')\n",
    "            else:\n",
    "                # Обработка остальных категорий данных\n",
    "                block[k] = v.text.strip().replace('\\xa0', ' ').replace('| DOCTORPITER', '')\n",
    "        except Exception:\n",
    "            # Обработка исключения при возникновении ошибки\n",
    "            block[k] = ''\n",
    "      \n",
    "    # Словарь месяцев для преобразования даты\n",
    "    months = {'января': '01',\n",
    "              'февраля': '02',\n",
    "              'марта': '03', \n",
    "              'апреля': '04',\n",
    "              'мая': '05', \n",
    "              'июня': '06',\n",
    "              'июля': '07', \n",
    "              'августа': '08', \n",
    "              'сентября': '09', \n",
    "              'октября': '10', \n",
    "              'ноября': '11', \n",
    "              'декабря': '12'}\n",
    "\n",
    "    # Преобразование даты в формат \"год-месяц-день\"\n",
    "    if block.get('дата'):\n",
    "        date_parts = block['дата'].split(' ')\n",
    "        if len(date_parts) == 3:\n",
    "            day, month, year = date_parts\n",
    "            if day.isdigit() and month in months and year.isdigit():\n",
    "                month_num = months[month]\n",
    "                if int(day) < 10:\n",
    "                    day = f'0{day}'\n",
    "                block['дата'] = f'{year}-{month_num}-{day}'\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_article(link: str) -> typing.Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Парсит информацию со страницы статьи.\n",
    "\n",
    "    Args:\n",
    "        link (str): Ссылка на страницу статьи.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: Словарь с данными статьи.\n",
    "    \"\"\"\n",
    "\n",
    "    # Создаем пустой словарь для хранения данных статьи\n",
    "    block = dict()\n",
    "\n",
    "    # Отправляем GET-запрос к странице статьи\n",
    "    req = session.get(link, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "\n",
    "    # Парсинг HTML страницы\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "     # Извлекаем данные со страницы и записываем в словарь\n",
    "    block['URL'] = link\n",
    "    block['название источника'] = 'Доктор Питер'\n",
    "    block['дата'] = soup.find('span', {'class': 'ds-article-header-date-and-stats__date'})\n",
    "    block['автор'] = soup.find('span', {'class': 'ds-article-footer-authors__author'})\n",
    "    block['название статьи'] = soup.find('title')\n",
    "    block['текст статьи'] = soup.find_all(\n",
    "        'div', \n",
    "        {'ds-block-text text-style-body-1 ds-article-content__block ds-article-content__block_text'}\n",
    "        )\n",
    "\n",
    "    # Вызов функции get_text для обработки данных и возвращение результата\n",
    "    return get_text(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nth_page(section: str, page_number: int) -> list:\n",
    "    \"\"\"\n",
    "    Получает ссылки на новости с указанной страницы раздела сайта https://doctorpiter.ru/\n",
    "    и возвращает список новостей.\n",
    "\n",
    "    Args:\n",
    "        section (str): Раздел сайта, например 'novosti', 'releases', \n",
    "        'zdorove', 'obraz-zhizni', 'pravilnoe-pitanie', 'tags/serdce/', 'tags/diagnostika'.\n",
    "        page_number (int): Номер страницы для извлечения новостей.\n",
    "\n",
    "    Returns:\n",
    "        list: Список новостей с указанной страницы раздела.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Формирование URL для запроса\n",
    "    url = f'https://doctorpiter.ru/{section}/page-{page_number}/'\n",
    "    \n",
    "    # Отправка GET-запроса на указанный URL с помощью сессии\n",
    "    req = session.get(url, headers={'User-Agent': ua.random})\n",
    "    \n",
    "    # Получение текста страницы\n",
    "    page = req.text\n",
    "    \n",
    "    # Создание объекта BeautifulSoup для парсинга HTML\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    # Поиск всех элементов новостей на странице\n",
    "    links = soup.find_all(('a', {'class': 'announce-inline-tile__content mr-d-2'}))\n",
    "    \n",
    "    # Регулярное выражение для поиска ссылок на новости\n",
    "    rex = r'(?<=href=\")(\\/(?:novosti|releases|zdorove|obraz-zhizni|pravilnoe-pitanie|tags\\/serdce\\/|tags\\/diagnostika)\\/[a-zA-Z0-9-]+\\/)'\n",
    "    \n",
    "    # Извлечение ссылок на новости и фильтрация\n",
    "    links = [re.search(rex, str(i))[0] for i in links if re.search(rex, str(i)) is not None]\n",
    "    links = [i for i in links if 'page-' not in i]\n",
    "    links = [f'https://doctorpiter.ru/{i}' for i in links]\n",
    "\n",
    "    news = []\n",
    "    \n",
    "    # Парсинг каждой новости по ссылке и добавление в список новостей\n",
    "    for l in links:\n",
    "        try: \n",
    "            news.append(parse_one_article(l))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_pages(section: str, n_pages: int) -> list:\n",
    "    \"\"\"\n",
    "    Получает новости из указанного раздела сайта https://doctorpiter.ru/ и возвращает список всех новостей.\n",
    "\n",
    "    Args:\n",
    "        section (str): Раздел сайта, например 'novosti', 'releases', \n",
    "        'zdorove', 'obraz-zhizni', 'pravilnoe-pitanie', 'tags/serdce/', 'tags/diagnostika'.\n",
    "        n_pages (int): Количество страниц для извлечения новостей.\n",
    "\n",
    "    Returns:\n",
    "        list: Список всех новостей из указанного раздела и страниц.\n",
    "    \"\"\"\n",
    "    \n",
    "    blocks = []\n",
    "    \n",
    "    # Итерация по страницам для получения новостей\n",
    "    for i in tqdm(range(1, n_pages+1)):\n",
    "        blocks.extend(get_nth_page(section, i))\n",
    "\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(section: str, n_pages: int) -> list:\n",
    "    \"\"\"\n",
    "    Функция для получения данных из разделов и сохранения их в CSV файл.\n",
    "\n",
    "    Args:\n",
    "        section (str): Раздел сайта, например 'novosti', 'releases', 'zdorove', \n",
    "        'obraz-zhizni', 'tags/pravilnoe-pitanie', 'tags/serdce', 'tags/diagnostika'.\n",
    "        n_pages (int): Количество страниц для извлечения новостей.\n",
    "\n",
    "    Returns:\n",
    "        sections (list): Список данных из разделов.\n",
    "    \"\"\"\n",
    "\n",
    "    # Получение данных из раздела\n",
    "    blocks = get_n_pages(section, n_pages)  \n",
    "\n",
    "    # Сохранение данных\n",
    "    if section == 'novosti':\n",
    "        first = pd.DataFrame(blocks)\n",
    "        first.to_csv('Doctor_Piter.csv', index=False)\n",
    "    else:\n",
    "        previous = pd.read_csv('Doctor_Piter.csv')\n",
    "        new = pd.DataFrame(blocks)\n",
    "        final = pd.concat([previous, new], ignore_index=True)\n",
    "        final.to_csv('Doctor_Piter.csv', index=False)\n",
    "\n",
    "    # Промежуточное сообщение о результатах скачивания\n",
    "    print(f'Данные из раздела \"{section}\" успешно скачаны и сохранены.')\n",
    "    \n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data('novosti', 203)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.extend(get_data('/tags/pravilnoe-pitanie', 97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление дубликатов\n",
    "df = pd.read_csv('Doctor_Piter.csv')\n",
    "df_unique = df.drop_duplicates()\n",
    "df_unique.to_csv('Doctor_Piter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Функция для преобразования данных из CSV файла в строку \n",
    "    по заданному формату и записи в текстовый файл.\n",
    "\n",
    "    Args:\n",
    "        name (str): Название файла без расширения.\n",
    "\n",
    "    Returns:\n",
    "        transformed_data (str): Преобразованные данные в виде строки.\n",
    "    \"\"\"\n",
    "\n",
    "    # Чтение данных из файла \n",
    "    data = pd.read_csv(f'{name}.csv')\n",
    "    data = data.loc[:, 'URL':].values.tolist()\n",
    "    for d in range(len(data)):\n",
    "        for w in range(len(data[d])):\n",
    "            # замена nan на пустую строку\n",
    "            if isinstance(data[d][w], float):\n",
    "                data[d][w] = ''\n",
    "\n",
    "    # Преобразование данных в строку по заданному формату\n",
    "    data = ['\\n'.join(map(str, d)) for d in data]\n",
    "    text = '\\n=====\\n'.join(data)\n",
    "\n",
    "\n",
    "    # Запись данных в текстовый файл\n",
    "    with open(f'{name}.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Doctor_Piter.csv')\n",
    "texts = df['текст статьи'].tolist()\n",
    "\n",
    "# Преобразование текста\n",
    "for i in range(len(texts)):\n",
    "    text = texts[i]\n",
    "    if not isinstance(text, float):\n",
    "        new_text = ''\n",
    "        for e in range(len(text)-1):\n",
    "            if text[e] in ['.', '!', '?'] and text[e+1] not in '0123456789,.!?':\n",
    "                new_text += text[e] + ' '\n",
    "            elif text[e].islower() and text[e+1].isupper():\n",
    "                new_text += text[e] + '.\\n'\n",
    "            else:\n",
    "                new_text += text[e]\n",
    "\n",
    "        # Добавление последнего символа к новой строке\n",
    "        new_text += text[-1]\n",
    "\n",
    "        # Замена исходного текста на преобразованный\n",
    "        texts[i] = new_text\n",
    "\n",
    "# Обновление столбца 'текст статьи' с преобразованными текстами\n",
    "df['текст статьи'] = texts\n",
    "\n",
    "# Сохранение обновленного датафрейма в CSV файл\n",
    "df.to_csv('Doctor_Piter.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_data('Doctor_Piter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install pymystem3\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from pymystem3 import Mystem\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_one_text(text: str) -> list: \n",
    "    \"\"\"\n",
    "    Функция принимает на вход текст в виде строки и \n",
    "    возвращает список предложений после предобработки.\n",
    "\n",
    "    Аргументы:\n",
    "        text: str - исходный текст для обработки\n",
    "\n",
    "    Возвращаемое значение:\n",
    "        list - список предложений после предобработки\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try: \n",
    "        # Удаление лишних пробелов и символов\n",
    "        text = text.replace(', ', ' ').replace(' - ', ' ').replace('\\n', ' ')\n",
    "\n",
    "        # Разделение текста на предложения\n",
    "        sentences = sent_tokenize(text, language='russian')\n",
    "\n",
    "        # Удаление ссылок и специальных символов\n",
    "        sentences = [re.sub(r'http\\S+', '', s) for s in sentences]\n",
    "        sentences = [re.sub(r'[^\\w\\s]', '', s) for s in sentences]\n",
    "\n",
    "        # Приведение к нижнему регистру\n",
    "        sentences = [s.lower() for s in sentences]\n",
    "\n",
    "        m = Mystem()\n",
    "\n",
    "        # Лемматизация слов\n",
    "        sentences = [''.join(\n",
    "            m.lemmatize(s)).replace('  ', ' ').replace('\\n', '') for s in sentences]\n",
    "        return sentences\n",
    "    \n",
    "    except Exception: \n",
    "        with open('errors.txt', 'a', encoding='utf-8') as file:\n",
    "            file.write(f'{text}\\n\\n')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(name: str) -> list: \n",
    "    \"\"\"\n",
    "    Функция для предобработки текстов .\n",
    "    \n",
    "    Args:\n",
    "        name (str): Название файла без расширения\n",
    "        \n",
    "    Returns:\n",
    "        list: Список лемматизированных предложений.\n",
    "    \"\"\"\n",
    "\n",
    "    texts = list(pd.read_csv(f'{name}.csv', encoding='utf-8')['текст статьи']) \n",
    "    texts = [preprocess_one_text(texts[t]) for t in tqdm(range(len(texts)))]\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess('Doctor_Piter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemmatized_text.txt' , 'w', encoding='utf-8') as file:\n",
    "    file.writelines(f\"{i}\\n\" for i in data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение графа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from itertools import combinations\n",
    "from tqdm.auto import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_occurrences_in_ngrams(\n",
    "        ngrams: list) -> typing.Dict[str, typing.Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Функция для подсчета встречаемости слов в n-граммах.\n",
    "    \n",
    "    Args:\n",
    "        ngrams (list): Список n-грамм.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Dict[str, int]]: Словарь с встречаемостью слов в n-граммах.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_counts = {}\n",
    "    all_pairs = []\n",
    "    \n",
    "    # Создание всех возможных пар слов из n-грамм\n",
    "    for n in ngrams: \n",
    "        all_pairs.extend(list(combinations(n, 2)))\n",
    "    \n",
    "    # Подсчет встречаемости слов в парах\n",
    "    for pair in tqdm(range(len(all_pairs))):\n",
    "        if all_pairs[pair][0] not in word_counts:\n",
    "            word_counts[all_pairs[pair][0]] = {}\n",
    "            word_counts[all_pairs[pair][0]][all_pairs[pair][1]] = 1\n",
    "        else:\n",
    "            if all_pairs[pair][1] not in word_counts[all_pairs[pair][0]]:\n",
    "                word_counts[all_pairs[pair][0]][all_pairs[pair][1]] = 1\n",
    "            else:\n",
    "                word_counts[all_pairs[pair][0]][all_pairs[pair][1]] += 1\n",
    "\n",
    "        if all_pairs[pair][1] not in word_counts:\n",
    "            word_counts[all_pairs[pair][1]] = {}\n",
    "            word_counts[all_pairs[pair][1]][all_pairs[pair][0]] = 1\n",
    "        else:\n",
    "            if all_pairs[pair][0] not in word_counts[all_pairs[pair][1]]:\n",
    "                word_counts[all_pairs[pair][1]][all_pairs[pair][0]] = 1\n",
    "            else:\n",
    "                word_counts[all_pairs[pair][1]][all_pairs[pair][0]] += 1\n",
    "\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(n: int,  k: int = 6869) -> list:\n",
    "    \"\"\"\n",
    "    Функция для создания n-грамм из списка лемматизированных предложений.\n",
    "    \n",
    "    Args:\n",
    "        n (int): Значение n для создания n-грамм.\n",
    "        k (int): Значение k для отбора k текстов.\n",
    "        \n",
    "    Returns:\n",
    "        list: Список n-грамм.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Чтение файла и изъятие данных для построения n-грамм\n",
    "    with open('lemmatized_text.txt', 'r', encoding='utf-8') as file:\n",
    "        sentences = file.read().replace('[', '').replace(']', '').replace(\"'\", '').split('\\n')[:-1]\n",
    "        # случайная выборка из k текстов\n",
    "        sentences = random.sample(sentences, k)\n",
    "        sentences = [s.split(', ') for s in sentences]\n",
    "\n",
    "    # Создание n-грамм \n",
    "    list_of_ngrams = []\n",
    "    for t in sentences:\n",
    "        for s in t: \n",
    "            list_of_ngrams.extend(list(ngrams(s.split(), n)))\n",
    "\n",
    "    print(f'Количество {n}-грамм: {len(list_of_ngrams)}')\n",
    "    return list_of_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install networkx\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(\n",
    "        data: typing.Dict[str, typing.Dict[str, int]]) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Функция строит граф на основе данных из словаря.\n",
    "    \n",
    "    Parameters:\n",
    "    data (Dict[str, Dict[str, int]]): Словарь с данными для построения графа.\n",
    "    \n",
    "    Returns:\n",
    "    nx.Graph: Построенный граф.\n",
    "    \"\"\"\n",
    "    # Добавление дуг в граф.\n",
    "    G = nx.Graph()\n",
    "    for source_node, target_nodes in tqdm(data.items()):\n",
    "        for target_node, weight in target_nodes.items():\n",
    "            G.add_edge(source_node, target_node, weight=weight)\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(G: nx.Graph, n: int) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Функция визуализирует граф G с помощью библиотеки NetworkX.\n",
    "    \n",
    "    Parameters:\n",
    "    G (nx.Graph): Граф для отображения.\n",
    "    n (int): Значение n для n-грамм.\n",
    "    \n",
    "    Returns:\n",
    "    nx.Graph: Отображенный граф.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(50, 50))\n",
    "    # Определение позиций узлов для отображения\n",
    "    pos = nx.spring_layout(G)  \n",
    "\n",
    "    # Визуализация графа\n",
    "    nx.draw(G, pos, with_labels=True, font_weight='bold', node_size=500, node_color='lightblue', font_size=10)\n",
    "\n",
    "    # Добавление подписей с весами на рёбрах\n",
    "    labels = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Сохранение нарисованного графа в формате jpeg\n",
    "    plt.savefig(f'{n}-grams.jpeg')\n",
    "    plt.show()\n",
    "    \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение графа для 2-грамм\n",
    "data_2 = count_word_occurrences_in_ngrams(create_ngrams(2, 100))\n",
    "G2 = build_graph(data_2)\n",
    "draw_graph(G2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение графа для 3-грамм\n",
    "data_3 = count_word_occurrences_in_ngrams(create_ngrams(3, 100))\n",
    "G3 = build_graph(data_3)\n",
    "draw_graph(G3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение графа для 7-грамм\n",
    "data_7 = count_word_occurrences_in_ngrams(create_ngrams(7, 100))\n",
    "G7 = build_graph(data_7)\n",
    "draw_graph(G7, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение вектора частот встречаемости его соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(\n",
    "        data: typing.Dict[str, typing.Dict[str, int]]) -> typing.Dict[str, list[int]]:\n",
    "    \"\"\"\n",
    "    Строит вектор частот встречаемости соседий слова,\n",
    "    заполняя нулями отсутствующие значения.\n",
    "\n",
    "    Args:\n",
    "        data (Dict[str, Dict[str, int]]): Словарь с данными о соседних словах \n",
    "        и их частоте встречаемости.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, list[int]: Словарь с векторами частот встречаемости слов.\n",
    "    \"\"\"\n",
    "    # Длина словаря\n",
    "    length = len(data)\n",
    "\n",
    "    # Пустой словарь для векторов\n",
    "    vectors = {}\n",
    "\n",
    "    # Построение вектора частот встречаемости соседей слова\n",
    "    for word, neighbors in tqdm(data.items()):\n",
    "        vectors[word] = sorted(list(neighbors.values()), reverse=True)\n",
    "\n",
    "        # Заполнение нулями отсутсвующие значения\n",
    "        if len(vectors[word]) < length:\n",
    "            while len(vectors[word]) != length:\n",
    "                vectors[word].append(0)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf(vectors: typing.Dict[str, list[int]]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Вычисляет коэффициент Ципфа для каждого слова и возвращает список слов,\n",
    "    у которых коэффициент больше чем в два раза выше среднего.\n",
    "\n",
    "    Args:\n",
    "        vectors (Dict[str, list[int]]): Словарь с векторами частот встречаемости слов.\n",
    "\n",
    "    Returns:\n",
    "        list: Список слов, у которых коэффициент Ципфа выше чем в два раза среднего.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Пустой словарь для показателя Ципфа для каждого слова словаря текста\n",
    "    zipf_values = {}\n",
    "\n",
    "    # Расчет показателя Ципфа как отношение частот первых двух самых частотных слов\n",
    "    for word, vector in tqdm(vectors.items()):\n",
    "        if vector[1] != 0:\n",
    "            zipf_values[word] = vector[0] / vector[1]\n",
    "        else:\n",
    "            zipf_values[word] = 0\n",
    "    \n",
    "    # Подсчет среднего значения показателя Ципфа\n",
    "    average = mean(list(zipf_values.values()))\n",
    "\n",
    "    # Нахождение слов, чей показатель Ципфа \n",
    "    # выше более чем в 2 раза среднего значения\n",
    "    high_than_average = [k for k, v in zipf_values.items() if v > 2 * average]\n",
    "\n",
    "    return high_than_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_2 = get_vector(data_2)\n",
    "zipf_2 = zipf(vectors_2)\n",
    "zipf_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_3 = get_vector(data_3)\n",
    "zipf_3 = zipf(vectors_3)\n",
    "zipf_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_7 = get_vector(data_7)\n",
    "zipf_7 = zipf(vectors_7)\n",
    "zipf_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поиск слов, у которых показатель Ципфа в два раза \n",
    "# больше среднего по ВСЕМ текстам (на примере 4-грамм)\n",
    "all_data_4 = count_word_occurrences_in_ngrams(create_ngrams(4))\n",
    "all_vectors_4 = get_vector(all_data_4)\n",
    "zipf_4 = zipf(all_vectors_4 )\n",
    "zipf_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Творческое задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение степени схожести текстов, написанные одним автором, через подсчет косинусного расстояния между текстами.\n",
    "\n",
    "Гипотеза: тексты одного автора должны быть похожи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import combinations\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_texts(author: str) -> typing.Tuple[typing.List[str], typing.List]:\n",
    "    \"\"\"\n",
    "    Векторизует тексты из файла и возвращает названия статей и их векторное представление.\n",
    "\n",
    "    Args:\n",
    "        author (str): Имя автора, для которого нужно найти статьи.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List]: Названия статей и их векторное представление.\n",
    "    \"\"\"\n",
    "\n",
    "    # Чтение текстов из файла и обработка\n",
    "    with open('lemmatized_text.txt', 'r', encoding='utf-8') as file:\n",
    "        texts = file.read().replace('[', '').replace(\n",
    "            ']', '').replace(\"'\", '').replace(', ', '').split('\\n')[:-1]\n",
    "\n",
    "    df = pd.read_csv('Doctor_Piter.csv')\n",
    "    df['лемматизированный текст статьи'] = texts\n",
    "\n",
    "    # Фильтрация датафрейма по имени автора\n",
    "    author_articles = df[df['автор'] == author]\n",
    "    \n",
    "    # Получение списка названий статей и текстов статей\n",
    "    titles = author_articles['название статьи'].tolist()\n",
    "    texts = author_articles['лемматизированный текст статьи'].tolist()\n",
    "\n",
    "    # Векторизация текстов\n",
    "    tfidf = TfidfVectorizer()\n",
    "    vectors = tfidf.fit_transform(texts)\n",
    "\n",
    "    return titles, vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_texts(author: str) -> typing.Tuple[typing.List[str], float]:\n",
    "    \"\"\"\n",
    "    Находит похожие тексты на основе косинусного расстояния между векторами текстов.\n",
    "\n",
    "    Args:\n",
    "        author (str): Имя автора, для которого нужно найти статьи.\n",
    "    Returns:\n",
    "        Tuple[List[str], float]: Список с названиями самых похожих статей \n",
    "        и средним сходством статей автора.\n",
    "    \"\"\"\n",
    "\n",
    "    # нахождение векторов текстов и их названий\n",
    "    titles, vectors = vectorized_texts(author)\n",
    "\n",
    "    # составление всех возможных пар текстов\n",
    "    pairs = list(combinations(range(len(titles)), 2))\n",
    "    pairs = [list(comb) for comb in pairs]\n",
    "\n",
    "    # пустой список для похожих текстов\n",
    "    similar_texts = []\n",
    "\n",
    "    # определение похожести текстов\n",
    "    for p in range(len(pairs)):\n",
    "        f = pairs[p][0]\n",
    "        s = pairs[p][1]\n",
    "        similar_texts.append([titles[f], \n",
    "                              titles[s], \n",
    "                              cosine_similarity(vectors[f].toarray(),\n",
    "                                                 vectors[s].toarray())[0][0]])\n",
    "\n",
    "    # самые похожие статьи\n",
    "    most_similar_texts = sorted(similar_texts, \n",
    "                                key=lambda x: x[2], \n",
    "                                reverse=True)[0]\n",
    "\n",
    "    # средний уровень похожести \n",
    "    med = mean([s[2] for s in similar_texts])\n",
    "    \n",
    "    return most_similar_texts, med\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cписок авторов, у которых больше 40 статей\n",
    "df = pd.read_csv('Doctor_Piter.csv')\n",
    "df['автор'] = df['автор'].str.replace('ё', 'е')\n",
    "authors = df.dropna(subset=['автор'])['автор'].value_counts()\n",
    "authors = authors[authors > 40].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нахождение показателя схожести текстов для каждого автора в списке\n",
    "for author in tqdm(authors):\n",
    "    most_similar_texts, med = find_similar_texts(author)\n",
    "\n",
    "    # Сохранение данных\n",
    "    with open('similarity_rate.txt', 'a', encoding='utf-8') as file:\n",
    "        file.writelines(f'{author}\\n')\n",
    "        file.writelines(f'{str(med)}\\n')\n",
    "        file.writelines(f'Самые похожие статьи автора: \"{most_similar_texts[0]}\" и \"{most_similar_texts[1]}\"'\\\n",
    "                            f' (косинусное расстояние -- {most_similar_texts[2]}).\\n')\n",
    "        file.writelines('=====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: тексты одного автора оказались непохожими, что объясняется:\n",
    "- либо слишком разной тематикой текстов, \n",
    "- либо неверным способом определения близости текстов, \n",
    "- либо отсутсвием стиля у автора. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
